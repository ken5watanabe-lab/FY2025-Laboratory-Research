{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f56bd8da-e15e-495f-bc8b-055b28bb511d",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd1c4e-911c-44f5-a927-8ba4fbb84cb3",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a0cd20-aca4-46a0-b847-a7f113dabfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib japanize-matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6844cb8-d048-4616-94d0-06057b662bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff79e1de-3088-471f-89db-47c106f5fa66",
   "metadata": {},
   "outputs": [],
   "source": [
    "! git clone https://github.com/google-deepmind/alphagenome.git\n",
    "! pip install ./alphagenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b82dca2-d7d6-4d2c-8360-1a37e47ea199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0e883-a468-48ee-8901-aefbfc8d8105",
   "metadata": {},
   "source": [
    "## .fa.gz -> .txt -> single_line.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00479cc-226e-4aa3-aef3-d41065a2384e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "\n",
    "# 2ç•ªã‹ã‚‰22ç•ªæŸ“è‰²ä½“ã€ãŠã‚ˆã³X, YæŸ“è‰²ä½“ã‚’å‡¦ç†ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "chromosomes = list(range(1, 23)) + ['X', 'Y']\n",
    "\n",
    "for n in chromosomes:\n",
    "    # èª­ã¿è¾¼ã‚€FASTAãƒ•ã‚¡ã‚¤ãƒ«åã‚’.fa.gzã«å¤‰æ›´\n",
    "    source_fa_file = f'references_GRCh37_chr{n}.fa.gz'\n",
    "    destination_txt_file = f'references_GRCh37_chr{n}.txt'\n",
    "    \n",
    "    print(f\"Processing {source_fa_file} -> {destination_txt_file}...\")\n",
    "    \n",
    "    try:\n",
    "        # 1. gzipãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ã£ã¦.fa.gzãƒ•ã‚¡ã‚¤ãƒ«ã‚’'rt'(ãƒ†ã‚­ã‚¹ãƒˆèª­ã¿è¾¼ã¿)ãƒ¢ãƒ¼ãƒ‰ã§é–‹ã\n",
    "        with gzip.open(source_fa_file, 'rt') as f_read:\n",
    "            # .read()ã§ãƒ•ã‚¡ã‚¤ãƒ«ã®ä¸­èº«ã‚’å…¨ã¦èª­ã¿è¾¼ã¿ã€å¤‰æ•°ã«ä¿å­˜ã™ã‚‹\n",
    "            file_content = f_read.read()\n",
    "        \n",
    "        # 2. ä¿å­˜å…ˆã¨ãªã‚‹.txtãƒ•ã‚¡ã‚¤ãƒ«ã‚’'w'(æ›¸ãè¾¼ã¿)ãƒ¢ãƒ¼ãƒ‰ã§é–‹ã\n",
    "        with open(destination_txt_file, 'w') as f_write:\n",
    "            # èª­ã¿è¾¼ã‚“ã å†…å®¹ã‚’ãã®ã¾ã¾æ›¸ãè¾¼ã‚€\n",
    "            f_write.write(file_content)\n",
    "        \n",
    "        print(f\"-> Success: Saved content of '{source_fa_file}' to '{destination_txt_file}'.\")\n",
    "    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"-> Error: Source file '{source_fa_file}' was not found. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"-> An unexpected error occurred: {e}\")\n",
    "\n",
    "print(\"\\nAll processing finished.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005e6797-556c-412d-bc74-15d79f658c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# 1ç•ªã‹ã‚‰22ç•ªæŸ“è‰²ä½“ã€ãŠã‚ˆã³X, YæŸ“è‰²ä½“ã‚’å‡¦ç†ã™ã‚‹ãƒªã‚¹ãƒˆ\n",
    "# The list of chromosomes to process, from 1 to 22, plus X and Y.\n",
    "chromosomes = list(range(1, 23)) + ['X', 'Y']\n",
    "for n in chromosomes:\n",
    "    source_file = f'references_GRCh37_chr{n}.txt'\n",
    "    \n",
    "    # ä¿å­˜ã™ã‚‹æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«å (å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…)\n",
    "    destination_file = f'references_GRCh37_chr{n}_single_line.txt'\n",
    "\n",
    "    print(f\"Processing {source_file}...\")\n",
    "\n",
    "    try:\n",
    "        sequence_lines = []\n",
    "        # é€šå¸¸ã®openã§ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’'r'(èª­ã¿è¾¼ã¿)ãƒ¢ãƒ¼ãƒ‰ã§é–‹ã\n",
    "        # Open the text file in read mode ('r') using the standard open function.\n",
    "        with open(source_file, 'r') as f_read:\n",
    "            # ãƒ•ã‚¡ã‚¤ãƒ«ã‚’1è¡Œãšã¤èª­ã¿è¾¼ã‚€\n",
    "            # Read the file line by line.\n",
    "            for line in f_read:\n",
    "                # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ('>'ã§å§‹ã¾ã‚‹è¡Œ)ã§ãªã‘ã‚Œã°ã€ãƒªã‚¹ãƒˆã«è¿½åŠ \n",
    "                # If the line does not start with '>' (it's not a header), add it to the list.\n",
    "                if not line.startswith('>'):\n",
    "                    # è¡Œæœ«ã®æ”¹è¡Œæ–‡å­—ãªã©ã‚’å‰Šé™¤ã—ã¦è¿½åŠ \n",
    "                    # Append the line to the list after removing leading/trailing whitespace (including newlines).\n",
    "                    sequence_lines.append(line.strip())\n",
    "        \n",
    "        # ãƒªã‚¹ãƒˆã«æ ¼ç´ã—ãŸå…¨ã¦ã®å¡©åŸºé…åˆ—ã®è¡Œã‚’ã€'' (ç©ºæ–‡å­—åˆ—)ã§é€£çµã—ã¦1è¡Œã®æ–‡å­—åˆ—ã«ã™ã‚‹\n",
    "        # Join all the sequence lines in the list into a single string with no separator.\n",
    "        sequence_single_line = ''.join(sequence_lines)\n",
    "\n",
    "        # 1è¡Œã«ç¹‹ã’ãŸé…åˆ—ã‚’æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã™\n",
    "        # Write the resulting single-line string to the destination file.\n",
    "        with open(destination_file, 'w') as f_write:\n",
    "            f_write.write(sequence_single_line)\n",
    "\n",
    "        print(f\"-> Success: Saved sequence-only content to '{destination_file}'.\")\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"-> Error: Source file '{source_file}' was not found. Skipping.\")\n",
    "    except Exception as e:\n",
    "        print(f\"-> An unexpected error occurred: {e}\")\n",
    "\n",
    "print(f\"\\nAll processing finished. Check the '{output_dir}/' directory for results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e737f62-158a-4949-b84b-be4b994a7ba0",
   "metadata": {},
   "source": [
    "## Deduplicate SNPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd97024-28e7-4786-af7d-40978835a22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# å‡¦ç†ã—ãŸã„æŸ“è‰²ä½“ã®ãƒªã‚¹ãƒˆ\n",
    "chromosomes = list(range(1, 23))\n",
    "\n",
    "# å…ƒã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã‚‹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¨ã€ä¿å­˜å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª\n",
    "source_directory = \"../genome-data/gwas-id/\"\n",
    "output_directory = \"../genome-data/gwas-id_deduplicated/\"\n",
    "\n",
    "# ä¿å­˜å…ˆã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒãªã‘ã‚Œã°ä½œæˆ\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "print(\"Starting deduplication process...\")\n",
    "\n",
    "for chrom in chromosomes:\n",
    "    source_file = os.path.join(source_directory, f\"gwas-id_chr{chrom}_genotypes.tsv\")\n",
    "    output_file = os.path.join(output_directory, f\"gwas-id_chr{chrom}_genotypes_dedup.tsv\")\n",
    "    \n",
    "    if os.path.exists(source_file):\n",
    "        try:\n",
    "            print(f\"Processing {source_file}...\")\n",
    "            # SNPã‚«ã‚¿ãƒ­ã‚°ã‚’èª­ã¿è¾¼ã‚€\n",
    "            snp_df = pd.read_csv(source_file, sep='\\t', low_memory=False)\n",
    "            \n",
    "            # POSåˆ—ã§é‡è¤‡ã—ã¦ã„ã‚‹è¡Œã‚’å‰Šé™¤ï¼ˆæœ€åˆã®è¡Œã‚’æ®‹ã™ï¼‰\n",
    "            snp_df_deduplicated = snp_df.drop_duplicates(subset=['POS'], keep='first')\n",
    "            \n",
    "            # æ–°ã—ã„ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦ä¿å­˜\n",
    "            snp_df_deduplicated.to_csv(output_file, sep='\\t', index=False)\n",
    "            \n",
    "            print(f\"-> Saved deduplicated file to {output_file}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"-> An error occurred while processing {source_file}: {e}\")\n",
    "    else:\n",
    "        print(f\"-> Source file not found: {source_file}\")\n",
    "\n",
    "print(\"\\nDeduplication process finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ea4a05-1a4e-4db5-9569-d6922e839c87",
   "metadata": {},
   "source": [
    "## Make a list of protein coding genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b063a1-1506-42a9-9b4f-4b26a9470b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gtf_path = \"gencode.v19.annotation.gtf\"\n",
    "ensg2symbol = dict()\n",
    "\n",
    "with open(gtf_path) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\"):\n",
    "            continue\n",
    "        fields = line.strip().split(\"\\t\")\n",
    "        if len(fields) != 9:\n",
    "            continue\n",
    "        chrom, feature_type, start, end, score, strand, frame, attr = fields[0], fields[2], fields[3], fields[4], fields[5], fields[6], fields[7], fields[8]\n",
    "        if feature_type != \"gene\":\n",
    "            continue\n",
    "        attr_dict = {}\n",
    "        for a in attr.split(\";\"):\n",
    "            a = a.strip()\n",
    "            if a:\n",
    "                k_v = a.split(\" \", 1)\n",
    "                if len(k_v) == 2:\n",
    "                    key, value = k_v\n",
    "                    attr_dict[key] = value.strip('\"')\n",
    "        ensg = attr_dict.get(\"gene_id\")\n",
    "        symbol = attr_dict.get(\"gene_name\")\n",
    "        if ensg and symbol:\n",
    "            ensg2symbol[ensg.split('.')[0]] = symbol  # versionless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210939a-4186-4dfe-b40b-3278565b2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_file = \"./chrX_5000/genes/chrX_protein_coding_genes.txt\"\n",
    "gene_symbols = set()\n",
    "with open(txt_file) as f:\n",
    "    for l in f:\n",
    "        gid = l.strip()\n",
    "        if not gid:\n",
    "            continue\n",
    "        gid0 = gid.split('.')[0]  #v versionless\n",
    "        if gid0 in ensg2symbol:\n",
    "            gene_symbols.add(ensg2symbol[gid0])\n",
    "\n",
    "print(f\"{len(gene_symbols)} gene symbols mapped from {txt_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8a8453-256d-431c-84c6-0c946019192d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bed_file = \"ucsc_expression.bed\"\n",
    "csv_file = \"chrX_protein_coding_filtered.csv\"\n",
    "\n",
    "# BED ã® Gene Symbol æŠ½å‡º\n",
    "bed_symbols = set()\n",
    "with open(bed_file) as bf:\n",
    "    for line in bf:\n",
    "        if line.strip() == \"\" or line.startswith(\"#\"):\n",
    "            continue\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        chr_field = parts[0]\n",
    "        if chr_field not in (\"chrX\", \"X\"):\n",
    "            continue\n",
    "        bed_symbols.add(parts[3])\n",
    "\n",
    "# CSV ã® Gene Symbol æŠ½å‡º\n",
    "csv_symbols = set()\n",
    "import csv\n",
    "with open(csv_file) as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for row in reader:\n",
    "        csv_symbols.add(row[\"gene_symbol\"])\n",
    "\n",
    "# æ¯”è¼ƒ\n",
    "matched = csv_symbols & bed_symbols\n",
    "unmatched = csv_symbols - bed_symbols\n",
    "print(f\"Matched: {len(matched)}\")\n",
    "print(f\"Unmatched: {len(unmatched)}\")\n",
    "print(\"Unmatched genes:\", unmatched)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1ed76f-1c0a-4c3d-88e1-ad217238bfc7",
   "metadata": {},
   "source": [
    "# Analysis performed using AlphaGenome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5804f656-fb1e-490b-a39c-25d0f4c478bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "import csv\n",
    "from alphagenome.data import genome\n",
    "from alphagenome.models import dna_client\n",
    "import pandas as pd  # æœ€å°é™ã®ä½¿ç”¨ï¼ˆCSVä¿å­˜ã®ã¿ï¼‰\n",
    "\n",
    "# --- ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•° ---\n",
    "def process_simulation_task(task_info):\n",
    "    start_pos, simulation_num, chrom_num, full_ref_seq, snp_positions_array, lfc_dir, snp_dir, ontology = task_info\n",
    "\n",
    "    max_retries = 3\n",
    "    initial_backoff = 2.0\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model = dna_client.create(API_KEY)\n",
    "\n",
    "            window_size = 1048576\n",
    "            end_pos = min(start_pos + window_size, len(full_ref_seq))\n",
    "            ref_sequence_slice = full_ref_seq[start_pos:end_pos]\n",
    "\n",
    "            if ref_sequence_slice.startswith('N' * 100): return None\n",
    "            if not set(ref_sequence_slice).issubset(set(\"ATGCN\")): return None\n",
    "\n",
    "            # SNPä½ç½®æŠ½å‡ºï¼ˆNumPyã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰\n",
    "            mask = (snp_positions_array >= start_pos + 1) & (snp_positions_array <= end_pos)\n",
    "            positions_in_window = snp_positions_array[mask]\n",
    "\n",
    "            if len(positions_in_window) == 0: return None\n",
    "\n",
    "            snp_count_to_generate = min(500, len(positions_in_window))\n",
    "            selected_positions = random.sample(list(positions_in_window), snp_count_to_generate)\n",
    "\n",
    "            alt_sequence_list = list(ref_sequence_slice[:1048575])  # ALTã¯1æ–‡å­—çŸ­ã\n",
    "            bases = \"ACGT\"\n",
    "            snp_records = []\n",
    "\n",
    "            for pos in selected_positions:\n",
    "                index = pos - start_pos - 1\n",
    "                if index >= len(alt_sequence_list): continue\n",
    "                original_ref = alt_sequence_list[index]\n",
    "                if original_ref == 'N': continue\n",
    "\n",
    "                new_alt = random.choice(bases)\n",
    "                alt_sequence_list[index] = new_alt\n",
    "                snp_records.append(f\"{pos}_{original_ref}_{new_alt}\")\n",
    "\n",
    "            alt_sequence_slice = \"\".join(alt_sequence_list)\n",
    "\n",
    "            min_snp_pos, max_snp_pos = min(selected_positions), max(selected_positions)\n",
    "            var_start_index = min_snp_pos - start_pos - 1\n",
    "            var_end_index = max_snp_pos - start_pos\n",
    "            var_ref_bases = ref_sequence_slice[var_start_index:var_end_index][:1048575]\n",
    "            var_alt_bases = alt_sequence_slice[var_start_index:var_end_index][:1048575]\n",
    "\n",
    "            interval = genome.Interval(chromosome=f\"chr{chrom_num}\", start=start_pos, end=end_pos)\n",
    "            variant = genome.Variant(\n",
    "                chromosome=f\"chr{chrom_num}\",\n",
    "                position=min_snp_pos,\n",
    "                reference_bases=var_ref_bases,\n",
    "                alternate_bases=var_alt_bases\n",
    "            )\n",
    "\n",
    "            outputs = model.predict_variant(\n",
    "                interval=interval, variant=variant, ontology_terms=[ontology],\n",
    "                requested_outputs=[dna_client.OutputType.RNA_SEQ]\n",
    "            )\n",
    "\n",
    "            if outputs.reference.rna_seq.values.shape[1] > 0:\n",
    "                ref_array = outputs.reference.rna_seq.values\n",
    "                alt_array = outputs.alternate.rna_seq.values\n",
    "                lfc_array = np.log2((alt_array + 1e-6) / (ref_array + 1e-6))\n",
    "\n",
    "                lfc_npz_path = os.path.join(lfc_dir, f\"output_chr{chrom_num}_{start_pos}_{simulation_num}.npz\")\n",
    "                np.savez_compressed(lfc_npz_path, lfc_data=lfc_array)\n",
    "\n",
    "                snp_csv_path = os.path.join(snp_dir, f\"snp_chr{chrom_num}_{start_pos}_{simulation_num}.csv\")\n",
    "                with open(snp_csv_path, 'w', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([\"SNP\"])\n",
    "                    for record in snp_records:\n",
    "                        writer.writerow([record])\n",
    "\n",
    "                return True\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            if (\"RESOURCE_EXHAUSTED\" in error_message or \"UNAVAILABLE\" in error_message) and attempt < max_retries - 1:\n",
    "                wait_time = initial_backoff * (2 ** attempt) + random.uniform(0, 1)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "if __name__ == '__main__':\n",
    "    API_KEY = 'xxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "    window_size = 1048576\n",
    "    num_simulations_per_window = 5000\n",
    "    for CHROMOSOME_NUM in [\"X\"]:\n",
    "        REF_FILE = f\"../genome-data/GRCh37/references_GRCh37_chr{CHROMOSOME_NUM}_single_line.txt\"\n",
    "        SNP_CATALOG_FILE = f\"../genome-data/gwas-id_deduplicated/gwas-id_chr{CHROMOSOME_NUM}_genotypes_dedup.tsv\"\n",
    "        ONTOLOGY = \"UBERON:0013756\"\n",
    "\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Starting analysis for Chromosome {CHROMOSOME_NUM}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        LFC_NPZ_OUTPUT_DIR = f\"./chr{CHROMOSOME_NUM}_5000/LFCdata\"\n",
    "        os.makedirs(LFC_NPZ_OUTPUT_DIR, exist_ok=True)\n",
    "        SNP_CSV_OUTPUT_DIR = f\"./chr{CHROMOSOME_NUM}_5000/SNPdata\"\n",
    "        os.makedirs(SNP_CSV_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with open(REF_FILE, 'r') as f:\n",
    "                full_ref_seq = f.read().strip()\n",
    "            chromosome_length = len(full_ref_seq)\n",
    "\n",
    "            snp_catalog_df = pd.read_csv(SNP_CATALOG_FILE, sep='\\t', usecols=[\"POS\"], low_memory=False)\n",
    "            snp_catalog_df['POS'] = pd.to_numeric(snp_catalog_df['POS'], errors='coerce')\n",
    "            snp_catalog_df.dropna(subset=['POS'], inplace=True)\n",
    "            snp_positions_array = snp_catalog_df['POS'].to_numpy(dtype=np.int64)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Data file not found for chr{CHROMOSOME_NUM}, skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        window_start_positions = list(range(0, chromosome_length - window_size, window_size))\n",
    "        tasks = [(\n",
    "            start, sim_num, CHROMOSOME_NUM, full_ref_seq, snp_positions_array,\n",
    "            LFC_NPZ_OUTPUT_DIR, SNP_CSV_OUTPUT_DIR, ONTOLOGY\n",
    "        ) for start in window_start_positions for sim_num in range(1, num_simulations_per_window + 1)]\n",
    "\n",
    "        num_processes = max(1, multiprocessing.cpu_count() - 8)\n",
    "        print(f\"Starting parallel processing for {len(tasks)} tasks with {num_processes} processes...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "            results = pool.map(process_simulation_task, tasks)\n",
    "\n",
    "        end_time = time.time()\n",
    "        success_count = sum(1 for r in results if r is not None)\n",
    "        print(f\"\\nTotal parallel processing for chr{CHROMOSOME_NUM} finished in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"Successfully completed {success_count} out of {len(tasks)} tasks.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"All chromosome analyses are complete.\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957f32d9-d968-4db1-9490-2e8e4326759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import multiprocessing\n",
    "import time\n",
    "import csv\n",
    "from alphagenome.data import genome\n",
    "from alphagenome.models import dna_client\n",
    "import pandas as pd  # æœ€å°é™ã®ä½¿ç”¨ï¼ˆCSVä¿å­˜ã®ã¿ï¼‰\n",
    "\n",
    "# --- ãƒ¯ãƒ¼ã‚«ãƒ¼é–¢æ•° ---\n",
    "def process_simulation_task(task_info):\n",
    "    start_pos, simulation_num, chrom_num, full_ref_seq, snp_positions_array, lfc_dir, snp_dir, ontology = task_info\n",
    "\n",
    "    max_retries = 3\n",
    "    initial_backoff = 2.0\n",
    "\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            model = dna_client.create(API_KEY)\n",
    "\n",
    "            window_size = 1048576\n",
    "            end_pos = min(start_pos + window_size, len(full_ref_seq))\n",
    "            ref_sequence_slice = full_ref_seq[start_pos:end_pos]\n",
    "\n",
    "            if not set(ref_sequence_slice).issubset(set(\"ATGCN\")): return None\n",
    "\n",
    "            # SNPä½ç½®æŠ½å‡ºï¼ˆNumPyã§ãƒ•ã‚£ãƒ«ã‚¿ï¼‰\n",
    "            mask = (snp_positions_array >= start_pos + 1) & (snp_positions_array <= end_pos)\n",
    "            positions_in_window = snp_positions_array[mask]\n",
    "\n",
    "            if len(positions_in_window) == 0: return None\n",
    "\n",
    "            snp_count_to_generate = min(500, len(positions_in_window))\n",
    "            selected_positions = random.sample(list(positions_in_window), snp_count_to_generate)\n",
    "\n",
    "            alt_sequence_list = list(ref_sequence_slice[:1048575])  # ALTã¯1æ–‡å­—çŸ­ã\n",
    "            bases = \"ACGT\"\n",
    "            snp_records = []\n",
    "\n",
    "            for pos in selected_positions:\n",
    "                index = pos - start_pos - 1\n",
    "                if index >= len(alt_sequence_list): continue\n",
    "                original_ref = alt_sequence_list[index]\n",
    "                if original_ref == 'N': continue\n",
    "\n",
    "                new_alt = random.choice(bases)\n",
    "                alt_sequence_list[index] = new_alt\n",
    "                snp_records.append(f\"{pos}_{original_ref}_{new_alt}\")\n",
    "\n",
    "            alt_sequence_slice = \"\".join(alt_sequence_list)\n",
    "\n",
    "            min_snp_pos, max_snp_pos = min(selected_positions), max(selected_positions)\n",
    "            var_start_index = min_snp_pos - start_pos - 1\n",
    "            var_end_index = max_snp_pos - start_pos\n",
    "            var_ref_bases = ref_sequence_slice[var_start_index:var_end_index][:1048575]\n",
    "            var_alt_bases = alt_sequence_slice[var_start_index:var_end_index][:1048575]\n",
    "\n",
    "            interval = genome.Interval(chromosome=f\"chr{chrom_num}\", start=start_pos, end=end_pos)\n",
    "            variant = genome.Variant(\n",
    "                chromosome=f\"chr{chrom_num}\",\n",
    "                position=min_snp_pos,\n",
    "                reference_bases=var_ref_bases,\n",
    "                alternate_bases=var_alt_bases\n",
    "            )\n",
    "\n",
    "            outputs = model.predict_variant(\n",
    "                interval=interval, variant=variant, ontology_terms=[ontology],\n",
    "                requested_outputs=[dna_client.OutputType.RNA_SEQ]\n",
    "            )\n",
    "\n",
    "            if outputs.reference.rna_seq.values.shape[1] > 0:\n",
    "                ref_array = outputs.reference.rna_seq.values\n",
    "                alt_array = outputs.alternate.rna_seq.values\n",
    "                lfc_array = np.log2((alt_array + 1e-6) / (ref_array + 1e-6))\n",
    "\n",
    "                lfc_npz_path = os.path.join(lfc_dir, f\"output_chr{chrom_num}_{start_pos}_{simulation_num}.npz\")\n",
    "                np.savez_compressed(lfc_npz_path, lfc_data=lfc_array)\n",
    "\n",
    "                snp_csv_path = os.path.join(snp_dir, f\"snp_chr{chrom_num}_{start_pos}_{simulation_num}.csv\")\n",
    "                with open(snp_csv_path, 'w', newline='') as f:\n",
    "                    writer = csv.writer(f)\n",
    "                    writer.writerow([\"SNP\"])\n",
    "                    for record in snp_records:\n",
    "                        writer.writerow([record])\n",
    "\n",
    "                return True\n",
    "\n",
    "            return None\n",
    "\n",
    "        except Exception as e:\n",
    "            error_message = str(e)\n",
    "            if (\"RESOURCE_EXHAUSTED\" in error_message or \"UNAVAILABLE\" in error_message) and attempt < max_retries - 1:\n",
    "                wait_time = initial_backoff * (2 ** attempt) + random.uniform(0, 1)\n",
    "                time.sleep(wait_time)\n",
    "            else:\n",
    "                return None\n",
    "\n",
    "    return None\n",
    "\n",
    "# --- ãƒ¡ã‚¤ãƒ³å‡¦ç† ---\n",
    "if __name__ == '__main__':\n",
    "    API_KEY = 'xxxxxxxxxxxxxxxxxxxxxxxxxx'\n",
    "    window_size = 1048576\n",
    "    num_simulations_per_window = 5000\n",
    "    for CHROMOSOME_NUM in [\"X\"]:\n",
    "        REF_FILE = f\"../genome-data/GRCh37/references_GRCh37_chr{CHROMOSOME_NUM}_single_line.txt\"\n",
    "        SNP_CATALOG_FILE = f\"../genome-data/gwas-id_deduplicated/gwas-id_chr{CHROMOSOME_NUM}_genotypes_dedup.tsv\"\n",
    "        ONTOLOGY = \"UBERON:0013756\"\n",
    "\n",
    "        print(\"=\"*70)\n",
    "        print(f\"Starting analysis for Chromosome {CHROMOSOME_NUM}\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        LFC_NPZ_OUTPUT_DIR = f\"./chr{CHROMOSOME_NUM}_5000/LFCdata\"\n",
    "        os.makedirs(LFC_NPZ_OUTPUT_DIR, exist_ok=True)\n",
    "        SNP_CSV_OUTPUT_DIR = f\"./chr{CHROMOSOME_NUM}_5000/SNPdata\"\n",
    "        os.makedirs(SNP_CSV_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "        try:\n",
    "            with open(REF_FILE, 'r') as f:\n",
    "                full_ref_seq = f.read().strip()\n",
    "            chromosome_length = len(full_ref_seq)\n",
    "\n",
    "            snp_catalog_df = pd.read_csv(SNP_CATALOG_FILE, sep='\\t', usecols=[\"POS\"], low_memory=False)\n",
    "            snp_catalog_df['POS'] = pd.to_numeric(snp_catalog_df['POS'], errors='coerce')\n",
    "            snp_catalog_df.dropna(subset=['POS'], inplace=True)\n",
    "            snp_positions_array = snp_catalog_df['POS'].to_numpy(dtype=np.int64)\n",
    "        except FileNotFoundError as e:\n",
    "            print(f\"Data file not found for chr{CHROMOSOME_NUM}, skipping. Error: {e}\")\n",
    "            continue\n",
    "\n",
    "        window_start_positions = list(range(0, min(3145728, chromosome_length - window_size), window_size))\n",
    "        tasks = [(\n",
    "            start, sim_num, CHROMOSOME_NUM, full_ref_seq, snp_positions_array,\n",
    "            LFC_NPZ_OUTPUT_DIR, SNP_CSV_OUTPUT_DIR, ONTOLOGY\n",
    "        ) for start in window_start_positions for sim_num in range(1, num_simulations_per_window + 1)]\n",
    "\n",
    "        num_processes = max(1, multiprocessing.cpu_count() - 8)\n",
    "        print(f\"Starting parallel processing for {len(tasks)} tasks with {num_processes} processes...\")\n",
    "        start_time = time.time()\n",
    "\n",
    "        with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "            results = pool.map(process_simulation_task, tasks)\n",
    "\n",
    "        end_time = time.time()\n",
    "        success_count = sum(1 for r in results if r is not None)\n",
    "        print(f\"\\nTotal parallel processing for chr{CHROMOSOME_NUM} finished in {end_time - start_time:.2f} seconds.\")\n",
    "        print(f\"Successfully completed {success_count} out of {len(tasks)} tasks.\")\n",
    "\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"All chromosome analyses are complete.\")\n",
    "    print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac16f0-57f9-4a50-9d9c-ae0c06903234",
   "metadata": {},
   "source": [
    "# Compute per-gene LFC averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfeffa32-4323-4643-9fa8-5194af3d9d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "extract_chrX_genes_singlewindow_region_preload_log.py\n",
    "\n",
    "ãƒªãƒ¼ã‚¸ãƒ§ãƒ³å˜ä½ãƒ—ãƒªãƒ­ãƒ¼ãƒ‰æ–¹å¼ï¼ˆè¦ªãƒ—ãƒ­ã‚»ã‚¹ã§ãã®ãƒªãƒ¼ã‚¸ãƒ§ãƒ³ã®å…¨ sim ã‚’èª­ã¿è¾¼ã¿ã€\n",
    "Pool ã‚’ä½œã£ã¦ãƒ¯ãƒ¼ã‚«ãƒ¼ã«å‡¦ç†ã•ã›ã‚‹ã€‚Linux ã® fork ã«ã‚ˆã‚‹ COW ã‚’åˆ©ç”¨ï¼‰\n",
    "\n",
    "- CSV_FILE ã«ã‚ã‚‹ Gene Symbol ã®ã¿å‡¦ç†\n",
    "- require_single ãƒ¢ãƒ¼ãƒ‰ï¼šéºä¼å­ãŒã¡ã‚‡ã†ã©1ã¤ã® window (= region) ã«å®Œå…¨ã«åã¾ã‚‹éºä¼å­ã®ã¿å‡¦ç†\n",
    "- å‡ºåŠ› per-gene npz: OUTPUT_DIR/<gene_symbol>.npz ã« 'LFC_sim' ã‚’ (num_sims,1) ã§ä¿å­˜\n",
    "- è¤‡æ•°ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã«ã¾ãŸãŒã‚‹éºä¼å­ã¯ç„¡è¦–ã—ã€æœ€å¾Œã«ãƒªã‚¹ãƒˆè¡¨ç¤º\n",
    "- ä¿å­˜ã•ã‚Œãªã‹ã£ãŸéºä¼å­ã‚„ãƒ‡ãƒ¼ã‚¿ãªã—éºä¼å­ã‚’ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§æ¨™æº–å‡ºåŠ›ã«è¡¨ç¤º\n",
    "\"\"\"\n",
    "import os, sys, glob, re, math, traceback\n",
    "import numpy as np\n",
    "from multiprocessing import Pool, cpu_count, get_start_method\n",
    "from collections import Counter\n",
    "import csv\n",
    "import gc\n",
    "\n",
    "# -------- CONFIG --------\n",
    "LFC_NPZ_DIR = \"chrX_5000/LFCdata\"\n",
    "BED_FILE = \"ucsc_expression.bed\"\n",
    "CSV_FILE = \"chrX_protein_coding_filtered.csv\"\n",
    "CHROMOSOME = \"X\"\n",
    "WINDOW_SIZE = 1048576\n",
    "OUTPUT_DIR = \"chrX_5000/genes\"\n",
    "NPROCS = min(max(1, cpu_count()), 8)\n",
    "# -----------------------\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- read CSV Gene Symbol list ---\n",
    "gene_symbols_set = set()\n",
    "with open(CSV_FILE) as cf:\n",
    "    reader = csv.DictReader(cf)\n",
    "    for row in reader:\n",
    "        gene_symbols_set.add(row[\"gene_symbol\"])\n",
    "print(f\"Loaded {len(gene_symbols_set)} gene symbols from {CSV_FILE}\")\n",
    "\n",
    "# --- read BED and filter for chrX and genes in gene_symbols_set ---\n",
    "bed_genes = []\n",
    "with open(BED_FILE) as bf:\n",
    "    for line in bf:\n",
    "        if line.strip() == \"\" or line.startswith(\"#\"):\n",
    "            continue\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        chr_field, start_s, end_s, name = parts[0], parts[1], parts[2], parts[3]\n",
    "        if chr_field not in (f\"chr{CHROMOSOME}\", CHROMOSOME):\n",
    "            continue\n",
    "        if name in gene_symbols_set:\n",
    "            bed_genes.append((name, int(start_s), int(end_s)))\n",
    "bed_genes.sort(key=lambda x: x[1])\n",
    "print(f\"Found {len(bed_genes)} genes in BED that match the gene list on chr{CHROMOSOME}\")\n",
    "\n",
    "if len(bed_genes) == 0:\n",
    "    print(\"No genes found to process. Check BED and gene list matching.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- parse LFC npz files ---\n",
    "def tolerant_parse(fp):\n",
    "    name = os.path.basename(fp).replace('.npz','')\n",
    "    ints = [int(x) for x in re.findall(r'(\\d+)', name)]\n",
    "    if len(ints) >= 2:\n",
    "        region = ints[0]\n",
    "        sim = ints[-1] - 1\n",
    "        return region, sim\n",
    "    return None\n",
    "\n",
    "lfc_files = sorted(glob.glob(os.path.join(LFC_NPZ_DIR, \"*.npz\")))\n",
    "sim_files = {}\n",
    "region_to_sims = {}\n",
    "region_starts = set()\n",
    "failed = 0\n",
    "for fp in lfc_files:\n",
    "    parsed = tolerant_parse(fp)\n",
    "    if parsed is None:\n",
    "        failed += 1\n",
    "        continue\n",
    "    region, sim = parsed\n",
    "    if sim < 0:\n",
    "        continue\n",
    "    sim_files[(region, sim)] = fp\n",
    "    region_starts.add(region)\n",
    "    region_to_sims.setdefault(region, set()).add(sim)\n",
    "region_starts = sorted(region_starts)\n",
    "print(f\"Parsed {len(sim_files)} LFC file entries, region_starts={len(region_starts)}, failed_parses={failed}\")\n",
    "\n",
    "if len(sim_files) == 0:\n",
    "    print(\"No LFC files parsed. Check LFC_NPZ_DIR and filename pattern.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "# --- helper ---\n",
    "GLOBAL_REGION_LFCS = None\n",
    "GLOBAL_REGION_START = None\n",
    "GLOBAL_NUM_SIMS = None\n",
    "GLOBAL_WINDOW_SIZE = WINDOW_SIZE\n",
    "GLOBAL_OUTPUT_DIR = OUTPUT_DIR\n",
    "\n",
    "def init_worker(region_arrays, region_start, num_sims):\n",
    "    global GLOBAL_REGION_LFCS, GLOBAL_REGION_START, GLOBAL_NUM_SIMS\n",
    "    GLOBAL_REGION_LFCS = region_arrays\n",
    "    GLOBAL_REGION_START = region_start\n",
    "    GLOBAL_NUM_SIMS = num_sims\n",
    "\n",
    "def process_gene_using_global(gene_tuple):\n",
    "    try:\n",
    "        gene_id, gstart, gend = gene_tuple\n",
    "        out_path = os.path.join(GLOBAL_OUTPUT_DIR, f\"{gene_id}.npz\")\n",
    "        if os.path.exists(out_path):\n",
    "            return (gene_id, \"skipped_exists\", None)\n",
    "\n",
    "        rs = GLOBAL_REGION_START\n",
    "        re = rs + GLOBAL_WINDOW_SIZE\n",
    "        if not (gstart >= rs and gend <= re):\n",
    "            return (gene_id, \"no_overlap_region\", None)\n",
    "\n",
    "        rel_start = gstart - rs\n",
    "        rel_end = gend - rs\n",
    "        out_arr = np.full((GLOBAL_NUM_SIMS, 1), np.nan, dtype=np.float32)\n",
    "        any_found = False\n",
    "\n",
    "        for sim_idx in range(GLOBAL_NUM_SIMS):\n",
    "            arr = GLOBAL_REGION_LFCS[sim_idx]\n",
    "            if arr is None or rel_end > len(arr):\n",
    "                continue\n",
    "            seg = arr[rel_start:rel_end]\n",
    "            if seg.size == 0 or np.all(np.isnan(seg)):\n",
    "                continue\n",
    "            out_arr[sim_idx, 0] = float(np.nanmean(seg))\n",
    "            any_found = True\n",
    "\n",
    "        if not any_found:\n",
    "            return (gene_id, \"no_data_found\", None)\n",
    "\n",
    "        np.savez_compressed(out_path, LFC_sim=out_arr)\n",
    "        return (gene_id, \"ok\", int(np.sum(np.isfinite(out_arr))))\n",
    "    except Exception as e:\n",
    "        tb = traceback.format_exc()\n",
    "        return (gene_id, \"worker_exception\", str(tb))\n",
    "\n",
    "# --- prepare genes by region ---\n",
    "genes_by_region = {}\n",
    "ignored_multiwindow_genes = []\n",
    "for gene in bed_genes:\n",
    "    name, s, e = gene\n",
    "    matched = False\n",
    "    for rs in region_starts:\n",
    "        if s >= rs and e <= rs + WINDOW_SIZE:\n",
    "            genes_by_region.setdefault(rs, []).append(gene)\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        ignored_multiwindow_genes.append(name)\n",
    "\n",
    "print(f\"Genes fully contained in regions (per-region counts):\")\n",
    "tot_contained = 0\n",
    "for rs in sorted(genes_by_region.keys()):\n",
    "    cnt = len(genes_by_region[rs])\n",
    "    tot_contained += cnt\n",
    "    print(f\"  region {rs}: {cnt} genes\")\n",
    "print(f\"Total genes fully contained: {tot_contained}\")\n",
    "print(f\"Total genes ignored upfront (not fully contained): {len(ignored_multiwindow_genes)}\")\n",
    "\n",
    "# --- multiprocessing start method ---\n",
    "start_method = get_start_method(allow_none=True)\n",
    "if start_method != 'fork':\n",
    "    try:\n",
    "        import multiprocessing as mp\n",
    "        mp.set_start_method('fork', force=True)\n",
    "        print(\"Set start method to 'fork' for better memory sharing.\")\n",
    "    except Exception:\n",
    "        print(\"Warning: multiprocessing start method is not 'fork'.\")\n",
    "\n",
    "# --- region loop ---\n",
    "all_results = []\n",
    "skipped_count = 0\n",
    "\n",
    "for region_start in region_starts:\n",
    "    genes_in_region = genes_by_region.get(region_start, [])\n",
    "    if not genes_in_region:\n",
    "        continue\n",
    "    sim_idx_set = sorted(region_to_sims.get(region_start, []))\n",
    "    if not sim_idx_set:\n",
    "        print(f\"Region {region_start}: no sim files, skipping.\")\n",
    "        continue\n",
    "    num_sims = max(sim_idx_set) + 1\n",
    "    print(f\"\\n=== Processing region {region_start} (num_sims inferred = {num_sims}) ===\")\n",
    "    print(f\"  Genes in region to process: {len(genes_in_region)}\")\n",
    "\n",
    "    # load sim arrays\n",
    "    region_arrays = [None] * num_sims\n",
    "    loaded = 0\n",
    "    try:\n",
    "        for sim in sim_idx_set:\n",
    "            fp = sim_files.get((region_start, sim))\n",
    "            if fp is None:\n",
    "                continue\n",
    "            with np.load(fp) as npz:\n",
    "                arr = npz.get('lfc_data', None)\n",
    "                if arr is None:\n",
    "                    continue\n",
    "                if arr.ndim == 2 and arr.shape[1] == 1:\n",
    "                    arr = arr[:,0]\n",
    "                region_arrays[sim] = np.ascontiguousarray(arr, dtype=np.float32)\n",
    "                loaded += 1\n",
    "        print(f\"  Loaded {loaded}/{len(sim_idx_set)} sim arrays for region {region_start}.\")\n",
    "    except MemoryError:\n",
    "        print(\"MemoryError while loading region arrays, skipping region.\")\n",
    "        del region_arrays\n",
    "        gc.collect()\n",
    "        continue\n",
    "\n",
    "    print(f\"  Creating pool with {NPROCS} workers for region {region_start} ...\")\n",
    "    pool = Pool(NPROCS, initializer=init_worker, initargs=(region_arrays, region_start, num_sims))\n",
    "    try:\n",
    "        results_region = pool.map(process_gene_using_global, genes_in_region)\n",
    "    finally:\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "\n",
    "    # log results\n",
    "    for gene_id, status, info in results_region:\n",
    "        all_results.append((gene_id, status, info))\n",
    "        if status == \"skipped_exists\":\n",
    "            print(f\"  Gene {gene_id}: already exists, skipped.\")\n",
    "        elif status == \"no_data_found\":\n",
    "            print(f\"  Gene {gene_id}: no LFC data found in region {region_start}, skipped.\")\n",
    "        elif status == \"worker_exception\":\n",
    "            print(f\"  Gene {gene_id}: worker exception:\\n{info}\")\n",
    "\n",
    "    del region_arrays\n",
    "    gc.collect()\n",
    "\n",
    "# add ignored multiwindow genes to results\n",
    "for g in ignored_multiwindow_genes:\n",
    "    if not any(r[0] == g for r in all_results):\n",
    "        all_results.append((g, \"multiwindow_skipped\", None))\n",
    "\n",
    "# --- write summary CSV ---\n",
    "summary_path = os.path.join(OUTPUT_DIR, \"_summary.csv\")\n",
    "with open(summary_path, \"w\") as oh:\n",
    "    oh.write(\"gene_id,status,info\\n\")\n",
    "    for gid, status, info in all_results:\n",
    "        oh.write(f\"{gid},{status},{info}\\n\")\n",
    "\n",
    "# --- summary ---\n",
    "cnt = Counter([s for (_, s, _) in all_results])\n",
    "print(\"\\nDone. Summary:\")\n",
    "for k,v in cnt.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "if ignored_multiwindow_genes:\n",
    "    print(f\"\\nIgnored genes spanning multiple windows (count={len(ignored_multiwindow_genes)}):\")\n",
    "    for g in ignored_multiwindow_genes:\n",
    "        print(g)\n",
    "\n",
    "print(\"Per-gene npz files written to:\", OUTPUT_DIR)\n",
    "print(\"Summary CSV:\", summary_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa18c8b-217f-4312-8172-9d3b31e88692",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1879a461-4eae-4c6b-a53b-5241994b81b4",
   "metadata": {},
   "source": [
    "## Find SNP-sensitive genes and Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25e269e-d96a-4114-acc2-eaf080d9f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "violinplot_topN_with_imputation.py\n",
    "\n",
    "- NPZ å†…ã® LFC é…åˆ—ã« NaN ãŒå«ã¾ã‚Œã¦ã„ãŸã‚‰ãƒ­ã‚°å‡ºåŠ›ã—ã€éºä¼å­å†…ã®å¹³å‡ã§è£œå®Œã—ã¦å‡¦ç†ã—ã¾ã™ã€‚\n",
    "- å…¨ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§ mean/std ã‚’è¨ˆç®— â†’ æ¨™æº–åå·®ä¸Šä½ TOP_N ã‚’é¸æŠ\n",
    "- ãƒ—ãƒ­ãƒƒãƒˆæ™‚ã®ã¿ NUM_SAMPLE å€‹ã‚’ãƒ©ãƒ³ãƒ€ãƒ æŠ½å‡ºã—ã¦violinplot + points è¡¨ç¤º\n",
    "- ç®±ã²ã’å›³ã‚’ PNGï¼ˆé«˜è§£åƒåº¦ï¼‰ã§ä¿å­˜: violin_topN.png\n",
    "\"\"\"\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "# ---------- CONFIG ----------\n",
    "BED_FILE = \"ucsc_expression.bed\"\n",
    "SUMMARY_FILE = \"chrX_5000/genes/_summary_updated.csv\"\n",
    "NPZ_DIR = \"chrX_5000/genes\"\n",
    "TOP_N = 10            # æ¨™æº–åå·®ä¸Šä½ä½•å€‹ã‚’è¡¨ç¤º\n",
    "NUM_SAMPLE = 100      # ãƒ—ãƒ­ãƒƒãƒˆç”¨ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ•°\n",
    "SEED = 1\n",
    "IMPUTE_METHOD = \"mean\"  # \"mean\" ã¾ãŸã¯ \"median\"\n",
    "# ----------------------------\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# --- BEDèª­ã¿è¾¼ã¿ï¼ˆé †åºä¿æŒï¼‰ ---\n",
    "bed_genes = []\n",
    "with open(BED_FILE) as f:\n",
    "    for line in f:\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":\n",
    "            continue\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        chrom, start, end, gene = parts[0], int(parts[1]), int(parts[2]), parts[3]\n",
    "        bed_genes.append((gene, start, end))\n",
    "bed_genes.sort(key=lambda x: x[1])\n",
    "gene_order = [g[0] for g in bed_genes]\n",
    "\n",
    "# --- summary èª­ã¿è¾¼ã¿ & ok ã¾ãŸã¯ skipped_exists ã®éºä¼å­ã®ã¿ ---\n",
    "ok_genes = set()\n",
    "with open(SUMMARY_FILE) as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        if row.get(\"status\") in (\"ok\", \"skipped_exists\"):\n",
    "            ok_genes.add(row.get(\"gene_id\"))\n",
    "\n",
    "# BEDã®é †åºã§å€™è£œéºä¼å­ãƒªã‚¹ãƒˆ\n",
    "plot_genes = [g for g in gene_order if g in ok_genes]\n",
    "print(f\"Candidate genes from BED+SUMMARY: {len(plot_genes)}\")\n",
    "\n",
    "# --- npz èª­ã¿è¾¼ã¿ï¼ˆå½¢çŠ¶ãƒã‚§ãƒƒã‚¯ãƒ»NaNæ¤œå‡ºâ†’è£œå®Œï¼‰ ---\n",
    "lfc_dict = {}\n",
    "nan_info = []\n",
    "missing_files = []\n",
    "shape_counts = defaultdict(int)\n",
    "\n",
    "for gene in plot_genes:\n",
    "    npz_path = os.path.join(NPZ_DIR, f\"{gene}.npz\")\n",
    "    if not os.path.exists(npz_path):\n",
    "        missing_files.append(gene)\n",
    "        continue\n",
    "    try:\n",
    "        with np.load(npz_path) as npz:\n",
    "            key = \"LFC_sim\" if \"LFC_sim\" in npz else (list(npz.keys())[0] if len(npz.keys()) > 0 else None)\n",
    "            if key is None:\n",
    "                missing_files.append(gene)\n",
    "                continue\n",
    "            arr = np.asarray(npz[key]).squeeze()\n",
    "            if arr.ndim != 1:\n",
    "                print(f\"Warning: {gene}.npz -> array ndim={arr.ndim}, skipping\")\n",
    "                missing_files.append(gene)\n",
    "                continue\n",
    "            n_nan = int(np.isnan(arr).sum())\n",
    "            if n_nan > 0:\n",
    "                if n_nan == arr.size:\n",
    "                    print(f\"[SKIP] {gene}: all {n_nan} values are NaN -> skipped\")\n",
    "                    missing_files.append(gene)\n",
    "                    continue\n",
    "                fill_val = float(np.nanmedian(arr)) if IMPUTE_METHOD == \"median\" else float(np.nanmean(arr))\n",
    "                print(f\"[IMPUTE] {gene}: {n_nan} NaN(s) found. Filling with {IMPUTE_METHOD}={fill_val:.6g}\")\n",
    "                arr = np.where(np.isnan(arr), fill_val, arr)\n",
    "                nan_info.append((gene, n_nan, arr.size))\n",
    "            lfc_dict[gene] = arr\n",
    "            shape_counts[len(arr)] += 1\n",
    "    except Exception as e:\n",
    "        print(f\"[ERROR] Failed to load {npz_path}: {e}\")\n",
    "        missing_files.append(gene)\n",
    "\n",
    "print(f\"Loaded {len(lfc_dict)} genes with usable LFC arrays.\")\n",
    "if missing_files:\n",
    "    print(f\"Missing/unusable files for {len(missing_files)} genes (examples): {missing_files[:10]}\")\n",
    "if nan_info:\n",
    "    print(f\"Imputed NaN for {len(nan_info)} genes (examples): {nan_info[:10]}\")\n",
    "\n",
    "# --- simulation length check & truncate ---\n",
    "unique_lengths = sorted(shape_counts.keys())\n",
    "if len(unique_lengths) == 0:\n",
    "    raise RuntimeError(\"No valid LFC arrays loaded.\")\n",
    "if len(unique_lengths) > 1:\n",
    "    print(f\"Warning: multiple simulation lengths detected: {unique_lengths}\")\n",
    "    min_len = min(unique_lengths)\n",
    "    print(f\"Truncating to {min_len}.\")\n",
    "    for g, arr in list(lfc_dict.items()):\n",
    "        if len(arr) > min_len:\n",
    "            lfc_dict[g] = arr[:min_len]\n",
    "else:\n",
    "    min_len = unique_lengths[0]\n",
    "    print(f\"All arrays length = {min_len}\")\n",
    "\n",
    "# --- mean/std è¨ˆç®— ---\n",
    "means = {g: float(np.mean(v)) for g, v in lfc_dict.items()}\n",
    "stds  = {g: float(np.std(v))  for g, v in lfc_dict.items()}\n",
    "\n",
    "# --- æ¨™æº–åå·®é™é †ã§ TOP_N é¸æŠ ---\n",
    "top_genes = sorted(lfc_dict.keys(), key=lambda g: stds[g], reverse=True)[:TOP_N]\n",
    "print(f\"Top {TOP_N} genes by std: {top_genes}\")\n",
    "\n",
    "# --- ãƒ€ã‚¦ãƒ³ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ—ãƒ­ãƒƒãƒˆã®ã¿ï¼‰ ---\n",
    "num_total_sims = min_len\n",
    "sample_indices = sorted(random.sample(range(num_total_sims), min(NUM_SAMPLE, num_total_sims)))\n",
    "plot_rows = [{\"éºä¼å­å\": g, \"LFC\": float(lfc_dict[g][i])}\n",
    "             for g in top_genes for i in sample_indices]\n",
    "df_plot = pd.DataFrame(plot_rows)\n",
    "\n",
    "# --- ç®±ã²ã’å›³ã«å¤‰æ›´ ---\n",
    "plt.figure(figsize=(max(8, TOP_N*1.8), 6))\n",
    "sns.set_context('poster')\n",
    "ax = sns.violinplot(x=\"éºä¼å­å\", y=\"LFC\", data=df_plot)\n",
    "sns.stripplot(x=\"éºä¼å­å\", y=\"LFC\", data=df_plot, color=\"black\", size=3, jitter=True, ax=ax)\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.xlabel(\"Genes\")\n",
    "plt.ylabel(\"LFC\")\n",
    "plt.title(f\"Downsampled LFC for top {TOP_N} genes by std dev\")\n",
    "plt.tight_layout()\n",
    "\n",
    "# ğŸ”½ é«˜è§£åƒåº¦PNGã§ä¿å­˜\n",
    "plt.savefig(\"violin_topN.png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# --- çµ±è¨ˆãƒ†ãƒ¼ãƒ–ãƒ« ---\n",
    "df_stats = pd.DataFrame({\n",
    "    \"éºä¼å­å\": top_genes,\n",
    "    \"å¹³å‡å€¤\": [means[g] for g in top_genes],\n",
    "    \"æ¨™æº–åå·®\": [stds[g] for g in top_genes],\n",
    "})\n",
    "print(\"\\n=== Top genes statistics (computed on all simulations, with imputation) ===\")\n",
    "display(df_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebf98d1-290d-422d-b16d-b6ee751e5f0c",
   "metadata": {},
   "source": [
    "## Calculate SD and RMSE and Making Figs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0edafa6a-07cd-40d6-80ff-1482843707a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# ====== è¨­å®š ======\n",
    "data_dir = \"chrX_5000/genes/\"\n",
    "n_list = [2, 3, 4, 5, 6, 7, 8, 9, 10,\n",
    "          20, 30, 40, 50, 60, 70, 80, 90, 100,\n",
    "          200, 300, 400, 500, 600, 700, 800, 900,\n",
    "          1000, 2000, 3000, 4000, 5000]\n",
    "seed = 123\n",
    "rng = np.random.default_rng(seed)\n",
    "output_png = \"sd_rmse_mean95ci_noscatter.png\"\n",
    "\n",
    "sns.set(style='ticks', context='poster')  # ğŸ‘ˆ è¦‹ã‚„ã™ãèª¿æ•´\n",
    "\n",
    "# ====== ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ (.npz only) ======\n",
    "files = sorted([os.path.join(data_dir, f) for f in os.listdir(data_dir)\n",
    "                if f.lower().endswith(\".npz\")])\n",
    "if len(files) == 0:\n",
    "    raise FileNotFoundError(\"No .npz files found in \" + data_dir)\n",
    "\n",
    "gene_names = [os.path.basename(f) for f in files]\n",
    "\n",
    "gene_list = []\n",
    "for f in files:\n",
    "    with np.load(f) as d:\n",
    "        key = list(d.keys())[0]\n",
    "        arr = np.asarray(d[key]).ravel()\n",
    "        gene_list.append(arr)\n",
    "gene_matrix = np.stack(gene_list, axis=0)\n",
    "G, M = gene_matrix.shape\n",
    "print(f\"Loaded {G} genes, simulations per gene = {M}\")\n",
    "\n",
    "true_means = np.nanmean(gene_matrix, axis=1)\n",
    "\n",
    "nan_genes = [gene_names[i] for i in range(G) if np.isnan(gene_matrix[i]).any()]\n",
    "if nan_genes:\n",
    "    print(f\"Number of genes with NaN: {len(nan_genes)}\")\n",
    "\n",
    "sd_scatter_x, sd_scatter_y = [], []\n",
    "rmse_scatter_x, rmse_scatter_y = [], []\n",
    "valid_ns = []\n",
    "\n",
    "for n in n_list:\n",
    "    if n > M:\n",
    "        continue\n",
    "\n",
    "    indices = rng.choice(M, size=n, replace=True)\n",
    "    sel = gene_matrix[:, indices]\n",
    "\n",
    "    per_gene_sd = np.nanstd(sel, axis=1, ddof=1)\n",
    "    per_gene_mean = np.nanmean(sel, axis=1)\n",
    "    per_gene_rmse = np.sqrt((per_gene_mean - true_means) ** 2)\n",
    "\n",
    "    sd_scatter_x.extend([n] * G)\n",
    "    sd_scatter_y.extend(per_gene_sd.tolist())\n",
    "    rmse_scatter_x.extend([n] * G)\n",
    "    rmse_scatter_y.extend(per_gene_rmse.tolist())\n",
    "\n",
    "    valid_ns.append(n)\n",
    "    #print(f\"Processed n={n}\")\n",
    "\n",
    "df_sd = pd.DataFrame({'n': sd_scatter_x, 'SD': sd_scatter_y}).dropna()\n",
    "df_rmse = pd.DataFrame({'n': rmse_scatter_x, 'RMSE': rmse_scatter_y}).dropna()\n",
    "\n",
    "# ====== ãƒ—ãƒ­ãƒƒãƒˆ ======\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "# --- ä¸Š: SD ---\n",
    "ax1 = plt.subplot(2,1,1)\n",
    "sns.lineplot(data=df_sd, x='n', y='SD',\n",
    "             ax=ax1,\n",
    "             ci=95,\n",
    "             #err_style='band',\n",
    "             marker='o',\n",
    "             markersize=8,\n",
    "             color='tab:blue',\n",
    "             #err_kws={'ecolor': 'black', 'capsize': 5},\n",
    "             markeredgecolor='k',\n",
    "             label='mean Â± 95%CI')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Number of selected simulations (n, log scale)') \n",
    "ax1.set_ylabel('SD of LFC')\n",
    "ax1.set_ylim(0.0, 0.1)\n",
    "ax1.set_title('Per-gene SD (mean Â± 95% CI)')\n",
    "ax1.grid(True, which='both', ls='--', alpha=0.3)\n",
    "ax1.legend()\n",
    "\n",
    "# --- ä¸‹: RMSE ---\n",
    "ax2 = plt.subplot(2,1,2, sharex=ax1)\n",
    "sns.lineplot(data=df_rmse, x='n', y='RMSE',\n",
    "             ax=ax2,\n",
    "             ci=95,\n",
    "             #err_style='band',\n",
    "             marker='o',\n",
    "             markersize=8,\n",
    "             color='tab:orange',\n",
    "             #err_kws={'ecolor': 'black', 'capsize': 5},\n",
    "             markeredgecolor='k',\n",
    "             label='mean Â± 95%CI')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_xlabel('Number of selected simulations (n, log scale)')\n",
    "ax2.set_ylabel('LFC RMSE\\n(vs full-sim mean)')\n",
    "ax2.set_ylim(0.0, 0.05)\n",
    "ax2.set_title('Per-gene RMSE (vs full-sim mean)')\n",
    "ax2.grid(True, which='both', ls='--', alpha=0.3)\n",
    "ax2.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(output_png, dpi=300)\n",
    "print(f\"Saved figure to {output_png}\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d65bb84-66c8-4e7a-94bd-13efe148033c",
   "metadata": {},
   "source": [
    "## Find top SNPs that influence gene expression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbec42c-b01e-4f03-908f-03ff75ed8da4",
   "metadata": {},
   "source": [
    "### Linear regression is used to narrow down the SNPs, and Random Forest is used to find SNP candidates that contribute to gene expression changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a38aeb-06f0-4ab0-8d56-2b8aa6edd355",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "multiwindow_snp_lfc_pipeline_numpy.py\n",
    "\n",
    "- pandasã‚’ä½¿ã‚ãšnumpyã®ã¿ã§å‡¦ç†ãƒ»ä¿å­˜\n",
    "- per-window CSVã¨å…¨ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦é›†ç´„CSVã‚’ä¿å­˜\n",
    "\"\"\"\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression, SGDRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from scipy import sparse\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "BED_FILE = \"ucsc_expression.bed\"\n",
    "NPZ_DIR = \"chrX_5000/genes\"\n",
    "SNP_DIR = \"chrX_5000/SNPdata\"\n",
    "WINDOW_SIZE = 1048576\n",
    "TOP_LINEAR_SNP = 500\n",
    "SEED = 42\n",
    "USE_SPARSE_THRESHOLD = 30000\n",
    "MAX_LINEAR_DENSE = 50000\n",
    "OUTPUT_DIR = \"windows_topSNPs_rf\"\n",
    "SAVE_PLOTS = False\n",
    "# -----------------------------------------\n",
    "\n",
    "np.random.seed(SEED)\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- load BED and restrict to chrX ---\n",
    "bed_genes = []\n",
    "with open(BED_FILE) as bf:\n",
    "    for line in bf:\n",
    "        if line.startswith(\"#\") or line.strip() == \"\":\n",
    "            continue\n",
    "        parts = line.strip().split(\"\\t\")\n",
    "        if len(parts) < 4:\n",
    "            continue\n",
    "        chrom, start_s, end_s, gene = parts[0], parts[1], parts[2], parts[3]\n",
    "        if chrom != \"chrX\":\n",
    "            continue\n",
    "        try:\n",
    "            start_i = int(start_s)\n",
    "            end_i = int(end_s)\n",
    "        except:\n",
    "            continue\n",
    "        bed_genes.append((start_i, end_i, gene))\n",
    "if len(bed_genes) == 0:\n",
    "    raise RuntimeError(\"No chrX genes found in BED_FILE\")\n",
    "\n",
    "# compute window starts\n",
    "min_start = min(s for s, e, g in bed_genes)\n",
    "max_end = max(e for s, e, g in bed_genes)\n",
    "window_starts = list(range((min_start // WINDOW_SIZE) * WINDOW_SIZE, max_end + WINDOW_SIZE, WINDOW_SIZE))\n",
    "print(f\"chrX windows to process: {len(window_starts)} windows from {window_starts[0]} to {window_starts[-1]}\")\n",
    "\n",
    "def genes_in_window(wstart):\n",
    "    wend = wstart + WINDOW_SIZE\n",
    "    return [g for s,e,g in bed_genes if s >= wstart and s < wend]\n",
    "\n",
    "# aggregate results\n",
    "all_results_rows = []\n",
    "all_stats = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for wstart in window_starts:\n",
    "    wend = wstart + WINDOW_SIZE\n",
    "    genes = genes_in_window(wstart)\n",
    "    if len(genes) == 0:\n",
    "        continue\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"Window {wstart}-{wend}: {len(genes)} genes\")\n",
    "\n",
    "    # --- load gene LFC arrays ---\n",
    "    lfc_list = []\n",
    "    valid_genes = []\n",
    "    for gene in genes:\n",
    "        npz_path = os.path.join(NPZ_DIR, f\"{gene}.npz\")\n",
    "        if not os.path.exists(npz_path):\n",
    "            continue\n",
    "        try:\n",
    "            with np.load(npz_path) as npz:\n",
    "                key = list(npz.keys())[0]\n",
    "                arr = np.asarray(npz[key]).ravel()\n",
    "        except:\n",
    "            continue\n",
    "        if arr.size == 0:\n",
    "            continue\n",
    "        if np.isnan(arr).any():\n",
    "            arr = np.where(np.isnan(arr), np.nanmean(arr), arr)\n",
    "        lfc_list.append(arr)\n",
    "        valid_genes.append(gene)\n",
    "    if len(lfc_list) == 0:\n",
    "        print(f\"[WARN] no usable gene LFC arrays in window {wstart}\")\n",
    "        continue\n",
    "    lfc_matrix = np.stack(lfc_list, axis=0)\n",
    "    G, M = lfc_matrix.shape\n",
    "    print(f\"Loaded LFC for {G} genes, simulations per gene = {M}\")\n",
    "\n",
    "    # y = sum(|LFC|)\n",
    "    y = np.sum(np.abs(lfc_matrix), axis=0)\n",
    "    print(f\"y mean={np.nanmean(y):.4g}, max={np.nanmax(y):.4g}\")\n",
    "\n",
    "    # --- load SNP files ---\n",
    "    snp_pattern = os.path.join(SNP_DIR, f\"snp_chrX_{wstart}_*.csv\")\n",
    "    snp_files = sorted(glob.glob(snp_pattern))\n",
    "    if len(snp_files) == 0:\n",
    "        print(f\"[INFO] no SNP files for window {wstart}\")\n",
    "        continue\n",
    "    print(f\"Found {len(snp_files)} SNP files\")\n",
    "\n",
    "    all_snps = []\n",
    "    sim_count = 0\n",
    "    for sf in snp_files:\n",
    "        try:\n",
    "            snps = np.loadtxt(sf, dtype=str, delimiter=\",\")\n",
    "            if snps.ndim == 0:\n",
    "                snps = np.array([snps])\n",
    "        except:\n",
    "            snps = np.array([])\n",
    "        all_snps.extend(snps.tolist())\n",
    "        sim_count += 1\n",
    "\n",
    "    unique_snps = sorted(set(all_snps))\n",
    "    num_unique = len(unique_snps)\n",
    "    total_occurrences = len(all_snps)\n",
    "    print(f\"Unique SNPs: {num_unique}, total occurrences: {total_occurrences}\")\n",
    "    if num_unique == 0:\n",
    "        continue\n",
    "\n",
    "    # build X\n",
    "    snp_to_idx = {s:i for i,s in enumerate(unique_snps)}\n",
    "    use_sparse = (num_unique > USE_SPARSE_THRESHOLD)\n",
    "    if not use_sparse:\n",
    "        X = np.zeros((sim_count, num_unique), dtype=np.int8)\n",
    "        for sim_idx, sf in enumerate(snp_files):\n",
    "            try:\n",
    "                snps = np.loadtxt(sf, dtype=str, delimiter=\",\")\n",
    "                if snps.ndim == 0:\n",
    "                    snps = np.array([snps])\n",
    "                for s in snps:\n",
    "                    X[sim_idx, snp_to_idx[s]] = 1\n",
    "            except:\n",
    "                continue\n",
    "    else:\n",
    "        rows, cols, data = [], [], []\n",
    "        for sim_idx, sf in enumerate(snp_files):\n",
    "            try:\n",
    "                snps = np.loadtxt(sf, dtype=str, delimiter=\",\")\n",
    "                if snps.ndim == 0:\n",
    "                    snps = np.array([snps])\n",
    "                for s in snps:\n",
    "                    idx = snp_to_idx.get(s)\n",
    "                    if idx is not None:\n",
    "                        rows.append(sim_idx)\n",
    "                        cols.append(idx)\n",
    "                        data.append(1)\n",
    "            except:\n",
    "                continue\n",
    "        if len(rows) == 0:\n",
    "            continue\n",
    "        X = sparse.coo_matrix((data, (rows, cols)), shape=(sim_count, num_unique), dtype=np.int8).tocsr()\n",
    "    print(f\"X shape: {X.shape}\")\n",
    "\n",
    "    # --- linear regression screening ---\n",
    "    coef = None\n",
    "    if (not use_sparse) and (num_unique <= MAX_LINEAR_DENSE):\n",
    "        try:\n",
    "            model_lin = LinearRegression()\n",
    "            model_lin.fit(X, y)\n",
    "            coef = model_lin.coef_\n",
    "        except:\n",
    "            pass\n",
    "    if coef is None:\n",
    "        sgd = SGDRegressor(penalty='l2', max_iter=2000, tol=1e-4, random_state=SEED)\n",
    "        sgd.fit(X, y)\n",
    "        coef = sgd.coef_\n",
    "\n",
    "    abscoef = np.abs(coef)\n",
    "    sorted_idx = np.argsort(abscoef)[::-1]\n",
    "    top_k = min(TOP_LINEAR_SNP, num_unique)\n",
    "    top_idx = sorted_idx[:top_k]\n",
    "    top_snps = [unique_snps[i] for i in top_idx]\n",
    "\n",
    "    # --- RandomForest on top SNPs ---\n",
    "    if sparse.issparse(X):\n",
    "        X_top = X[:, top_idx].toarray()\n",
    "    else:\n",
    "        X_top = X[:, top_idx]\n",
    "    rf = RandomForestRegressor(n_estimators=200, random_state=SEED, n_jobs=-1)\n",
    "    rf.fit(X_top, y)\n",
    "    importances = rf.feature_importances_\n",
    "\n",
    "    # --- collect results ---\n",
    "    res = sorted(zip(top_snps, importances, coef[top_idx]), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # --- numpy arrayã§per-windowä¿å­˜ ---\n",
    "    res_array = np.array([[r[0], r[1], r[2]] for r in res], dtype=object)\n",
    "    out_csv = os.path.join(OUTPUT_DIR, f\"window_{wstart}_topSNPs_rf.csv\")\n",
    "    np.savetxt(out_csv, res_array, fmt=\"%s\", delimiter=\",\",\n",
    "               header=\"SNP,RF_importance,linear_coef\", comments='')\n",
    "    print(f\"Saved per-window results: {out_csv}\")\n",
    "\n",
    "    # --- append to global results ---\n",
    "    for snp, imp, lincoef in res:\n",
    "        pos = int(snp.split(\"_\")[0]) if \"_\" in snp else None\n",
    "        all_results_rows.append([wstart, snp, pos, imp, lincoef, num_unique, total_occurrences, G])\n",
    "\n",
    "    all_stats.append([wstart, G, num_unique, total_occurrences, float(np.nanmean(y)), float(np.nanmax(y))])\n",
    "\n",
    "# --- save aggregated results ---\n",
    "if len(all_results_rows) > 0:\n",
    "    all_results_array = np.array(all_results_rows, dtype=object)\n",
    "    out_all_csv = os.path.join(OUTPUT_DIR, \"all_windows_topSNPs_rf.csv\")\n",
    "    np.savetxt(out_all_csv, all_results_array, fmt=\"%s\", delimiter=\",\",\n",
    "               header=\"window_start,SNP,pos,RF_importance,linear_coef,num_unique_snps,total_snp_occurrences,num_genes_in_window\",\n",
    "               comments='')\n",
    "    print(f\"\\nSaved aggregated results to {out_all_csv}\")\n",
    "\n",
    "# --- save stats ---\n",
    "if len(all_stats) > 0:\n",
    "    all_stats_array = np.array(all_stats, dtype=object)\n",
    "    out_stats_csv = os.path.join(OUTPUT_DIR, \"windows_stats_summary.csv\")\n",
    "    np.savetxt(out_stats_csv, all_stats_array, fmt=\"%s\", delimiter=\",\",\n",
    "               header=\"window_start,num_genes,num_unique_snps,total_snp_occurrences,y_mean,y_max\", comments='')\n",
    "    print(f\"Saved window stats to {out_stats_csv}\")\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "print(f\"\\nFinished processing. elapsed {elapsed:.1f} sec\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bbd65-5273-40fd-afaf-c10130485102",
   "metadata": {},
   "source": [
    "### Make a Fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d964516-8f2a-4ba8-a307-19f19b049f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "plot_windows_rf.py\n",
    "\n",
    "- Read per-window CSV files produced by the pipeline in OUTPUT_DIR = \"windows_topSNPs_rf\"\n",
    "- For each window file window_{wstart}_topSNPs_rf.csv:\n",
    "    - plot SNP position vs RF_importance and save window_{wstart}_rf_scatter.png\n",
    "- After per-window plotting, aggregate all windows and save an all-windows scatter:\n",
    "    - all_windows_rf_scatter.png\n",
    "\"\"\"\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "OUTPUT_DIR = \"windows_topSNPs_rf\"\n",
    "PER_WINDOW_PATTERN = os.path.join(OUTPUT_DIR, \"window_*_topSNPs_rf.csv\")\n",
    "PER_WINDOW_PNG_TEMPLATE = os.path.join(OUTPUT_DIR, \"window_{wstart}_rf_scatter.png\")\n",
    "ALL_PNG = os.path.join(OUTPUT_DIR, \"all_windows_rf_scatter.png\")\n",
    "\n",
    "sns.set(style='ticks', context='poster')\n",
    "\n",
    "# find CSV files\n",
    "csv_files = sorted(glob.glob(PER_WINDOW_PATTERN))\n",
    "if len(csv_files) == 0:\n",
    "    raise FileNotFoundError(f\"No per-window CSVs found with pattern {PER_WINDOW_PATTERN}\")\n",
    "\n",
    "all_positions = []\n",
    "all_importances = []\n",
    "all_windows = []\n",
    "\n",
    "print(f\"Found {len(csv_files)} per-window CSV files. Processing...\")\n",
    "\n",
    "for csv_path in csv_files:\n",
    "    try:\n",
    "        # try reading with numpy genfromtxt (skip header)\n",
    "        data = np.genfromtxt(csv_path, delimiter=\",\", dtype=str, skip_header=1)\n",
    "    except Exception as e:\n",
    "        print(f\"[WARN] failed to read {csv_path}: {e}\")\n",
    "        continue\n",
    "\n",
    "    # handle cases: single-line file (returns 1d array) or empty\n",
    "    if data.size == 0:\n",
    "        print(f\"[INFO] empty or no-data file, skipping: {csv_path}\")\n",
    "        continue\n",
    "\n",
    "    # Ensure 2D\n",
    "    if data.ndim == 1:\n",
    "        data = data.reshape(1, -1)\n",
    "\n",
    "    # Expect columns: SNP,RF_importance,linear_coef\n",
    "    # SNP is data[:,0], RF_importance is data[:,1]\n",
    "    snp_col = data[:, 0].astype(str)\n",
    "    try:\n",
    "        rf_col = data[:, 1].astype(float)\n",
    "    except Exception:\n",
    "        # try to clean commas or quotes\n",
    "        rf_col = np.array([float(x.strip().strip('\"').strip(\"'\")) if x not in [None, \"nan\", \"\"] else np.nan for x in data[:,1]])\n",
    "\n",
    "    # extract positions (take numeric prefix before first underscore)\n",
    "    positions = []\n",
    "    for s in snp_col:\n",
    "        try:\n",
    "            pos = int(s.split(\"_\")[0])\n",
    "        except Exception:\n",
    "            # if cannot parse, skip this SNP entry\n",
    "            pos = None\n",
    "        positions.append(pos)\n",
    "\n",
    "    # filter invalid\n",
    "    pos_arr = np.array([p for p in positions if p is not None], dtype=int)\n",
    "    imp_arr = np.array([rf_col[i] for i,p in enumerate(positions) if p is not None], dtype=float)\n",
    "\n",
    "    if pos_arr.size == 0:\n",
    "        print(f\"[INFO] no valid SNP positions in {csv_path}, skipping plotting for this file.\")\n",
    "        continue\n",
    "\n",
    "    # per-window plotting\n",
    "    # determine window start from filename: window_{wstart}_topSNPs_rf.csv\n",
    "    fname = os.path.basename(csv_path)\n",
    "    try:\n",
    "        # parse numeric chunk after \"window_\" and before \"_top\"\n",
    "        prefix = fname.split(\"_topSNPs_rf\")[0]\n",
    "        wstart_str = prefix.replace(\"window_\", \"\")\n",
    "        wstart = int(wstart_str)\n",
    "    except Exception:\n",
    "        wstart = None\n",
    "\n",
    "    # 1) per-window scatter plot\n",
    "    plt.figure(figsize=(10,3))\n",
    "    plt.scatter(pos_arr, imp_arr, s=18, alpha=0.7, edgecolors='none')\n",
    "    plt.xlabel(\"Genomic position (bp)\")\n",
    "    plt.ylabel(\"RF importance\")\n",
    "    title = f\"Window {wstart}\" if wstart is not None else fname\n",
    "    plt.title(title)\n",
    "    if wstart is not None:\n",
    "        plt.xlim(wstart, wstart + 1048576)  # window size hard-coded as in pipeline\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    png_path = PER_WINDOW_PNG_TEMPLATE.format(wstart=wstart if wstart is not None else fname.replace(\".csv\",\"\"))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png_path, dpi=200)\n",
    "    plt.close()\n",
    "    print(f\"Saved per-window plot: {png_path}\")\n",
    "\n",
    "    # append to aggregate lists (for summary plot)\n",
    "    all_positions.extend(pos_arr.tolist())\n",
    "    all_importances.extend(imp_arr.tolist())\n",
    "    all_windows.extend([wstart] * len(pos_arr))\n",
    "\n",
    "# --- summary plot across all windows ---\n",
    "if len(all_positions) == 0:\n",
    "    print(\"No SNP data collected from per-window CSVs; skipping aggregate plot.\")\n",
    "else:\n",
    "    all_positions = np.array(all_positions, dtype=int)\n",
    "    all_importances = np.array(all_importances, dtype=float)\n",
    "    all_windows = np.array(all_windows, dtype=object)\n",
    "\n",
    "    # Sort by position for nicer plotting (optional)\n",
    "    order = np.argsort(all_positions)\n",
    "    all_positions = all_positions[order]\n",
    "    all_importances = all_importances[order]\n",
    "    all_windows = all_windows[order]\n",
    "\n",
    "    plt.figure(figsize=(18,4))\n",
    "    plt.scatter(all_positions, all_importances, s=12, alpha=0.5, edgecolors='none')\n",
    "    plt.xlabel(\"Genomic position (bp)\")\n",
    "    plt.ylabel(\"RF importance\")\n",
    "    plt.title(\"All windows: SNP RF importance across chrX (preliminary)\")\n",
    "    plt.grid(True, linestyle='--', alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(ALL_PNG, dpi=200)\n",
    "    plt.show()\n",
    "    print(f\"Saved aggregate plot: {ALL_PNG}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91662583-89a8-4388-9c67-da71301b7f83",
   "metadata": {},
   "source": [
    "## Make a Fig(Privacy Risk - Number of Simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403894bd-a480-4960-948f-a746dff32573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "sns.set(style='ticks', context='poster')\n",
    "\n",
    "# ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "n = 2200       # ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦å†…SNPæ•°\n",
    "x = 375        # 1å›ã®ã‚¤ãƒ³ãƒ—ãƒƒãƒˆã§å¤‰æ›´ã™ã‚‹SNPæ•°\n",
    "\n",
    "# ãƒªã‚¹ã‚¯é–¾å€¤pã®ç¯„å›²ï¼ˆå¯¾æ•°ç©ºé–“ï¼‰\n",
    "p_vals = np.logspace(-50, -1, 200)\n",
    "y_vals = np.log(p_vals) / np.log(1 - (x/n))\n",
    "\n",
    "# --- äº¤ç‚¹ã‚’æ±‚ã‚ã‚‹é–¢æ•° ---\n",
    "def find_p_for_y(target_y, p_vals, y_vals):\n",
    "    idx = np.argmin(np.abs(y_vals - target_y))\n",
    "    return p_vals[idx], y_vals[idx]\n",
    "\n",
    "# äº¤ç‚¹ã‚’æ±‚ã‚ã‚‹\n",
    "p25, y25 = find_p_for_y(25, p_vals, y_vals)\n",
    "p250, y250 = find_p_for_y(250, p_vals, y_vals)\n",
    "\n",
    "# --- ãƒ—ãƒ­ãƒƒãƒˆ ---\n",
    "plt.figure(figsize=(10.5,6))\n",
    "plt.plot(p_vals, y_vals, color='blue', lw=2, label=\"y = log(p) / log(1 - x/n)\")\n",
    "\n",
    "# y=25, y=250 ã®æ°´å¹³ç·š\n",
    "plt.axhline(25, color='orange', linestyle='--', label='y = 25')\n",
    "plt.axhline(250, color='green', linestyle='--', label='y = 250')\n",
    "\n",
    "# äº¤ç‚¹ï¼ˆç¸¦ç·šï¼‹ç‚¹ãƒãƒ¼ã‚«ãƒ¼ï¼‰\n",
    "plt.axvline(p25, color='orange', linestyle=':')\n",
    "plt.axvline(p250, color='green', linestyle=':')\n",
    "plt.scatter([p25, p250], [y25, y250], color='red', zorder=5)\n",
    "\n",
    "# äº¤ç‚¹ãƒ©ãƒ™ãƒ«ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨è¨˜ï¼‰\n",
    "plt.text(p25, y25 + 5, f\"p â‰ˆ {p25*100:.3g}%\", color='black', ha='left')\n",
    "plt.text(p250, y250 + 5, f\"p â‰ˆ {p250*100:.3g}%\", color='black', ha='left')\n",
    "\n",
    "# è»¸ã¨ã‚¿ã‚¤ãƒˆãƒ«\n",
    "plt.xscale('log')\n",
    "plt.xlabel(\"Risk threshold p (log scale)\")\n",
    "plt.ylabel(\"Required number of \\n input sequences y\")\n",
    "plt.title(\"Required y vs risk threshold p (x = 375, n = 2200)\")\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"risk assessment.png\", dpi=300)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
